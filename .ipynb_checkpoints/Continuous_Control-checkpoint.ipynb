{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient (DDPG)\n",
    "---\n",
    "This notebook implements a DDPG agent with Unity's Udacity-modified Reacher environment.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent\n",
    "\n",
    "Initialize the environment in the code cell below. \n",
    "\n",
    "Change the __*visible_environment*__ variable to enable training with (True) or without (False) a visible environment.\n",
    "\n",
    "The output below indicates the utilization of a cpu or a gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda:0\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "# set these parameters\n",
    "visible_environment = True \n",
    "one_agent = True # if false, then 20 agent environment\n",
    "\n",
    "if visible_environment == True and one_agent == True:\n",
    "    env = UnityEnvironment(file_name='../unity_environments/Reacher/Vis_one_agent/Reacher.x86_64')\n",
    "elif visible_environment == False and one_agent == True:\n",
    "    env = UnityEnvironment(file_name='../unity_environments/Reacher/NoVis_one_agent/Reacher.x86_64')\n",
    "elif visible_environment == True and one_agent == False:\n",
    "    env = UnityEnvironment(file_name='../unity_environments/Reacher/Vis_20_agents/Reacher.x86_64')\n",
    "elif visible_environment == False and one_agent == False:\n",
    "    env = UnityEnvironment(file_name='../unity_environments/Reacher/NoVis_20_agents/Reacher.x86_64')    \n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "\n",
    "# number of actions and states\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with DDPG\n",
    "\n",
    "Run the code cell below to train the agent from scratch.\n",
    "\n",
    "Alternatively, **skip** to the next step below (**4. Watch a Smart Agent!**), to load the saved model weights from a pre-trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/firstlinux/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.00\n",
      "Episode 2\tAverage Score: 0.00\n",
      "Episode 3\tAverage Score: 0.25\n",
      "Episode 3\tAverage Score: 0.25\n",
      "\n",
      "Environment solved in 3 episodes!\tAverage Score: 0.25\n",
      "\t Time to train network: 12.46 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_scores(scores, fig_indicator=\"normal\"):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.ylim((-4, 24))\n",
    "    plt.show()\n",
    "    \n",
    "    fig_name = \"Results/Figure_\" + fig_indicator + \".png\"\n",
    "    fig.savefig(fig_name)\n",
    "    \n",
    "    np.save(\"Results/scores_\" + fig_indicator + \".npy\", scores)\n",
    "    \n",
    "\n",
    "def ddpg(n_episodes=2000, max_t=700):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    max_score = -np.Inf\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        agent.reset()\n",
    "        score = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        for t in range(max_t):\n",
    "            \n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]        # send the action to the environment\n",
    "\n",
    "            next_states = env_info.vector_observations   # get the next state\n",
    "            rewards = env_info.rewards                   # get the reward\n",
    "            dones = env_info.local_done                  # see if episode has finished\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            score += rewards\n",
    "            \n",
    "            if np.any(dones):\n",
    "                break \n",
    "        scores_deque.append(np.mean(score))\n",
    "        scores.append(np.mean(score))\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))   \n",
    "        if i_episode % 100 == 0 or np.mean(scores_deque) >= 30.0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))   \n",
    "        if np.mean(scores_deque)>=30.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break            \n",
    "    return scores    \n",
    "\n",
    "t = time.time()\n",
    "scores = ddpg()\n",
    "elapsed = time.time() - t # https://stackoverflow.com/questions/5849800/what-is-the-python-equivalent-of-matlabs-tic-and-toc-functions\n",
    "print('\\t Time to train network: {:.2f}'.format(elapsed), 'seconds')\n",
    "\n",
    "plot_scores(scores, fig_indicator=\"ddpg_normal_soln_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch a Smart Agent!\n",
    "\n",
    "The code below loads the trained weights from file to watch a smart agent. \n",
    "\n",
    "To visualize the trained environment,\n",
    " - change the __*visible_environment*__ variable to **True** in (**2. Instantiate the Environment and Agent**)\n",
    " - restart the kernel, and \n",
    " - **skip** the previous section (**3. Train the Agent with DQN**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# load the weights from file\n",
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "\n",
    "for i in range(1):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    for j in range(200):\n",
    "        actions = agent.act(states)\n",
    "        env_info = env.step(actions)[brain_name]        # send the action to the environment\n",
    "        states = env_info.vector_observations\n",
    "        dones = env_info.local_done                  # see if episode has finished\n",
    "        \n",
    "        if np.any(dones):\n",
    "            break     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
